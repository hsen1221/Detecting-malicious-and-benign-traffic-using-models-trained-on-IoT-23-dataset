{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21f5ed78-98f2-4f67-bdfa-dd36821e3fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inspecting file structure...\n",
      "#separator \\x09\n",
      "#set_separator\t,\n",
      "#empty_field\t(empty)\n",
      "#unset_field\t-\n",
      "#path\tconn\n",
      "Starting balanced dataset creation...\n",
      "Available RAM: 7.0GB\n",
      "[2/23] Sampling 1.labeled (0.1GB) -> 124143 rows...\n",
      "\n",
      "Error: Usecols do not match columns, columns expected but not found: ['history', 'ts', 'resp_pkts', 'id.orig_p', 'conn_state', 'local_orig', 'duration', 'label', 'id.resp_h', 'orig_bytes', 'resp_ip_bytes', 'id.resp_p', 'service', 'orig_pkts', 'orig_ip_bytes', 'uid', 'local_resp', 'proto', 'missed_bytes', 'id.orig_h', 'resp_bytes']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import resample\n",
    "from joblib import dump\n",
    "import time\n",
    "import psutil\n",
    "\n",
    "# =============================================\n",
    "# 1. Configuration (ADJUST AS NEEDED)\n",
    "# =============================================\n",
    "DATA_DIR = \".\"  # Directory with 23 files\n",
    "# Add this right after DATA_DIR definition\n",
    "print(\"\\nInspecting file structure...\")\n",
    "sample_file = [f for f in os.listdir(DATA_DIR) if f.endswith(\".labeled\")][0]\n",
    "with open(os.path.join(DATA_DIR, sample_file), 'r') as f:\n",
    "    for _ in range(5):  # Print first 5 lines\n",
    "        print(f.readline().strip())\n",
    "OUTPUT_FILE = \"iot23_balanced_2gb_noSMOTE.joblib\"  # Output filename\n",
    "TARGET_SIZE_GB = 1.8  # Target dataset size (1.5-2GB)\n",
    "MAX_RAM_USAGE = 9  # GB (2GB buffer for system)\n",
    "\n",
    "# Essential columns to keep (reduces memory)\n",
    "ESSENTIAL_COLS = [\n",
    "    \"ts\", \"uid\", \"id.orig_h\", \"id.orig_p\", \"id.resp_h\", \"id.resp_p\",\n",
    "    \"proto\", \"service\", \"duration\", \"orig_bytes\", \"resp_bytes\",\n",
    "    \"conn_state\", \"local_orig\", \"local_resp\", \"missed_bytes\",\n",
    "    \"history\", \"orig_pkts\", \"orig_ip_bytes\", \"resp_pkts\", \"resp_ip_bytes\",\n",
    "    \"label\"\n",
    "]\n",
    "\n",
    "# =============================================\n",
    "# 2. Memory-Optimized File Sampling\n",
    "# =============================================\n",
    "def sample_with_memory_control(filepath, target_rows):\n",
    "    \"\"\"Samples large files without full memory load\"\"\"\n",
    "    # Get approximate row count\n",
    "    with open(filepath, 'r') as f:\n",
    "        total_lines = sum(1 for line in f if not line.startswith('#'))\n",
    "    \n",
    "    # Dynamic chunksize (1% of file or 100k max)\n",
    "    chunksize = min(100000, max(50000, int(total_lines * 0.01)))\n",
    "    samples = []\n",
    "    \n",
    "    for chunk in pd.read_csv(\n",
    "        filepath,\n",
    "        sep=\"\\t\",\n",
    "        comment=\"#\",\n",
    "        chunksize=chunksize,\n",
    "        usecols=ESSENTIAL_COLS,\n",
    "        low_memory=True\n",
    "    ):\n",
    "        # Sample 5-15% from each chunk\n",
    "        sample_frac = min(0.15, max(0.05, target_rows/(total_lines * 2)))\n",
    "        samples.append(chunk.sample(frac=sample_frac, random_state=42))\n",
    "        \n",
    "        # Early termination\n",
    "        if sum(len(s) for s in samples) >= target_rows * 0.9:\n",
    "            break\n",
    "    \n",
    "    # Combine and return exact target size\n",
    "    combined = pd.concat(samples)\n",
    "    return combined.sample(min(target_rows, len(combined)), random_state=42)\n",
    "\n",
    "# =============================================\n",
    "# 3. Balanced Dataset Creation\n",
    "# =============================================\n",
    "def create_balanced_dataset():\n",
    "    print(\"Starting balanced dataset creation...\")\n",
    "    start_time = time.time()\n",
    "    mem = psutil.virtual_memory()\n",
    "    print(f\"Available RAM: {mem.available/(1024**3):.1f}GB\")\n",
    "    \n",
    "    # --------------------------------------------------\n",
    "    # Phase 1: Sample from all files with memory control\n",
    "    # --------------------------------------------------\n",
    "    all_samples = []\n",
    "    target_per_file = 120000  # Initial target rows per file\n",
    "    \n",
    "    for i, filename in enumerate(sorted(os.listdir(DATA_DIR))):\n",
    "        if not filename.endswith(\".labeled\"):\n",
    "            continue\n",
    "            \n",
    "        filepath = os.path.join(DATA_DIR, filename)\n",
    "        file_size_gb = os.path.getsize(filepath)/(1024**3)\n",
    "        \n",
    "        # Adjust sample size based on file size\n",
    "        file_target = int(target_per_file * min(2, 1 + file_size_gb/4))\n",
    "        \n",
    "        print(f\"[{i+1}/23] Sampling {filename} ({file_size_gb:.1f}GB) -> {file_target} rows...\")\n",
    "        sample = sample_with_memory_control(filepath, file_target)\n",
    "        all_samples.append(sample)\n",
    "        \n",
    "        # Memory safeguard\n",
    "        current_ram = psutil.virtual_memory().used/(1024**3)\n",
    "        if current_ram > MAX_RAM_USAGE:\n",
    "            print(f\"Memory alert ({current_ram:.1f}GB). Reducing future samples.\")\n",
    "            target_per_file = int(target_per_file * 0.7)\n",
    "    \n",
    "    # --------------------------------------------------\n",
    "    # Phase 2: Combine and preprocess\n",
    "    # --------------------------------------------------\n",
    "    print(\"\\nCombining samples...\")\n",
    "    combined = pd.concat(all_samples, ignore_index=True)\n",
    "    del all_samples  # Free memory\n",
    "    \n",
    "    # Memory optimization\n",
    "    for col in [\"duration\", \"orig_bytes\", \"resp_bytes\"]:\n",
    "        combined[col] = pd.to_numeric(combined[col], downcast=\"float\")\n",
    "    \n",
    "    # Categorical encoding\n",
    "    for col in [\"proto\", \"service\", \"conn_state\"]:\n",
    "        combined[col] = combined[col].astype(\"category\").cat.codes\n",
    "    \n",
    "    # --------------------------------------------------\n",
    "    # Phase 3: Manual class balancing\n",
    "    # --------------------------------------------------\n",
    "    print(\"\\nBalancing classes without SMOTE...\")\n",
    "    \n",
    "    # Get class distribution\n",
    "    class_counts = combined[\"label\"].value_counts()\n",
    "    min_class = class_counts.idxmin()\n",
    "    min_count = class_counts.min()\n",
    "    \n",
    "    balanced_dfs = []\n",
    "    \n",
    "    # Downsample majority classes\n",
    "    for class_name in class_counts.index:\n",
    "        class_df = combined[combined[\"label\"] == class_name]\n",
    "        if len(class_df) > min_count * 1.2:  # Only downsample if significantly larger\n",
    "            class_df = class_df.sample(min_count, random_state=42)\n",
    "        balanced_dfs.append(class_df)\n",
    "    \n",
    "    final_df = pd.concat(balanced_dfs, ignore_index=True)\n",
    "    \n",
    "    # --------------------------------------------------\n",
    "    # Phase 4: Final size adjustment\n",
    "    # --------------------------------------------------\n",
    "    # Calculate memory usage\n",
    "    final_size = final_df.memory_usage(deep=True).sum() / (1024**3)\n",
    "    \n",
    "    if final_size > TARGET_SIZE_GB * 1.1:\n",
    "        reduction = TARGET_SIZE_GB / final_size\n",
    "        final_df = final_df.sample(frac=reduction, random_state=42)\n",
    "        final_size = final_df.memory_usage(deep=True).sum() / (1024**3)\n",
    "    \n",
    "    print(f\"\\nFinal Dataset Stats:\")\n",
    "    print(f\"- Size: {final_size:.2f}GB\")\n",
    "    print(f\"- Samples: {len(final_df):,}\")\n",
    "    print(f\"- Class Distribution:\\n{final_df['label'].value_counts()}\")\n",
    "    \n",
    "    # Prepare for saving\n",
    "    X = final_df.drop(\"label\", axis=1)\n",
    "    y = final_df[\"label\"]\n",
    "    \n",
    "    # Save\n",
    "    dump({\n",
    "        \"features\": X,\n",
    "        \"labels\": y,\n",
    "        \"feature_names\": list(X.columns),\n",
    "        \"metadata\": {\n",
    "            \"source_files\": os.listdir(DATA_DIR),\n",
    "            \"creation_time\": pd.Timestamp.now(),\n",
    "            \"balance_method\": \"manual_downsampling\"\n",
    "        }\n",
    "    }, OUTPUT_FILE)\n",
    "    \n",
    "    # Performance report\n",
    "    total_time = (time.time() - start_time)/60\n",
    "    print(f\"\\nProcessing completed in {total_time:.1f} minutes\")\n",
    "    print(f\"Dataset saved to {OUTPUT_FILE}\")\n",
    "\n",
    "# =============================================\n",
    "# 4. Execute with error handling\n",
    "# =============================================\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        create_balanced_dataset()\n",
    "    except MemoryError:\n",
    "        print(\"\\nMemory Error! Try reducing target_per_file in config\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa56f138-427c-4e56-93be-cfe88dde9055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting balanced dataset creation...\n",
      "Available RAM: 7.3GB\n",
      "[1/22] Sampling 1.labeled (0.1GB) -> 124143 rows...\n",
      "[2/22] Sampling 17conn.log.labeled (7.6GB) -> 240000 rows...\n",
      "[3/22] Sampling 20conn.log.labeled (0.0GB) -> 120011 rows...\n",
      "[4/22] Sampling 21.labeled (0.0GB) -> 120011 rows...\n",
      "[5/22] Sampling 3.labeled (0.0GB) -> 120681 rows...\n",
      "[6/22] Sampling 33conn.log.labeled (7.3GB) -> 240000 rows...\n",
      "[7/22] Sampling 34conn.log.labeled (0.0GB) -> 120084 rows...\n",
      "[8/22] Sampling 35conn.log.labeled (1.2GB) -> 157444 rows...\n",
      "[9/22] Sampling 36conn.log.labeled (1.7GB) -> 169952 rows...\n",
      "[10/22] Sampling 39conn.log.labeled (10.1GB) -> 240000 rows...\n",
      "[11/22] Sampling 42conn.log.labeled (0.0GB) -> 120016 rows...\n",
      "[12/22] Sampling 43conn.log.labeled (8.7GB) -> 240000 rows...\n",
      "[13/22] Sampling 44conn.log.labeled (0.0GB) -> 120000 rows...\n",
      "[14/22] Sampling 48conn.log.labeled (0.5GB) -> 134856 rows...\n",
      "[15/22] Sampling 49conn.log.labeled (0.8GB) -> 143850 rows...\n",
      "[16/22] Sampling 4conn.log.labeled (0.0GB) -> 120001 rows...\n",
      "[17/22] Sampling 52conn.log.labeled (2.9GB) -> 205783 rows...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hussein Salloum\\AppData\\Local\\Temp\\ipykernel_7652\\2910480186.py:46: DtypeWarning: Columns (8,9,10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in pd.read_csv(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18/22] Sampling 5conn.log.labeled (0.0GB) -> 120005 rows...\n",
      "[19/22] Sampling 60conn.log.labeled (0.4GB) -> 133063 rows...\n",
      "[20/22] Sampling 7conn.log.labeled (0.0GB) -> 120000 rows...\n",
      "[21/22] Sampling 8conn.log.labeled (0.0GB) -> 120039 rows...\n",
      "[22/22] Sampling 9conn.log.labeled (0.9GB) -> 147756 rows...\n",
      "\n",
      "Combining samples...\n",
      "\n",
      "Error: Unable to parse string \"-\" at position 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import resample\n",
    "from joblib import dump\n",
    "import time\n",
    "import psutil\n",
    "\n",
    "# =============================================\n",
    "# 1. Configuration (ADJUST AS NEEDED)\n",
    "# =============================================\n",
    "DATA_DIR = \".\"  # Directory with 23 files\n",
    "OUTPUT_FILE = \"iot23_balanced_2gb_noSMOTE.joblib\"  # Output filename\n",
    "TARGET_SIZE_GB = 1.8  # Target dataset size (1.5-2GB)\n",
    "MAX_RAM_USAGE = 9  # GB (2GB buffer for system)\n",
    "\n",
    "# Essential columns to keep (reduces memory)\n",
    "ESSENTIAL_COLS = [\n",
    "    \"ts\", \"uid\", \"id.orig_h\", \"id.orig_p\", \"id.resp_h\", \"id.resp_p\",\n",
    "    \"proto\", \"service\", \"duration\", \"orig_bytes\", \"resp_bytes\",\n",
    "    \"conn_state\", \"local_orig\", \"local_resp\", \"missed_bytes\",\n",
    "    \"history\", \"orig_pkts\", \"orig_ip_bytes\", \"resp_pkts\", \"resp_ip_bytes\",\n",
    "    \"label\"\n",
    "]\n",
    "\n",
    "# =============================================\n",
    "# 2. Memory-Optimized File Sampling\n",
    "# =============================================\n",
    "def sample_with_memory_control(filepath, target_rows):\n",
    "    \"\"\"Samples large files without full memory load\"\"\"\n",
    "    # First read the header to get column names\n",
    "    with open(filepath, 'r') as f:\n",
    "        for line in f:\n",
    "            if line.startswith('#fields'):\n",
    "                columns = line.strip().split('\\t')[1:]  # Get column names after #fields\n",
    "                break\n",
    "    \n",
    "    # Get approximate row count\n",
    "    with open(filepath, 'r') as f:\n",
    "        total_lines = sum(1 for line in f if not line.startswith('#'))\n",
    "    \n",
    "    # Dynamic chunksize (1% of file or 100k max)\n",
    "    chunksize = min(100000, max(50000, int(total_lines * 0.01)))\n",
    "    samples = []\n",
    "    \n",
    "    for chunk in pd.read_csv(\n",
    "        filepath,\n",
    "        sep=\"\\t\",\n",
    "        comment=\"#\",\n",
    "        chunksize=chunksize,\n",
    "        names=columns,  # Use the columns we extracted\n",
    "        usecols=[col for col in ESSENTIAL_COLS if col in columns],  # Only use existing columns\n",
    "        low_memory=True\n",
    "    ):\n",
    "        # Sample 5-15% from each chunk\n",
    "        sample_frac = min(0.15, max(0.05, target_rows/(total_lines * 2)))\n",
    "        samples.append(chunk.sample(frac=sample_frac, random_state=42))\n",
    "        \n",
    "        # Early termination\n",
    "        if sum(len(s) for s in samples) >= target_rows * 0.9:\n",
    "            break\n",
    "    \n",
    "    # Combine and return exact target size\n",
    "    combined = pd.concat(samples)\n",
    "    return combined.sample(min(target_rows, len(combined)), random_state=42)\n",
    "\n",
    "# =============================================\n",
    "# 3. Balanced Dataset Creation\n",
    "# =============================================\n",
    "def create_balanced_dataset():\n",
    "    print(\"Starting balanced dataset creation...\")\n",
    "    start_time = time.time()\n",
    "    mem = psutil.virtual_memory()\n",
    "    print(f\"Available RAM: {mem.available/(1024**3):.1f}GB\")\n",
    "    \n",
    "    # --------------------------------------------------\n",
    "    # Phase 1: Sample from all files with memory control\n",
    "    # --------------------------------------------------\n",
    "    all_samples = []\n",
    "    target_per_file = 120000  # Initial target rows per file\n",
    "    \n",
    "    labeled_files = [f for f in sorted(os.listdir(DATA_DIR)) if f.endswith(\".labeled\")]\n",
    "    if not labeled_files:\n",
    "        raise FileNotFoundError(\"No .labeled files found in directory\")\n",
    "    \n",
    "    for i, filename in enumerate(labeled_files):\n",
    "        filepath = os.path.join(DATA_DIR, filename)\n",
    "        file_size_gb = os.path.getsize(filepath)/(1024**3)\n",
    "        \n",
    "        # Adjust sample size based on file size\n",
    "        file_target = int(target_per_file * min(2, 1 + file_size_gb/4))\n",
    "        \n",
    "        print(f\"[{i+1}/{len(labeled_files)}] Sampling {filename} ({file_size_gb:.1f}GB) -> {file_target} rows...\")\n",
    "        sample = sample_with_memory_control(filepath, file_target)\n",
    "        all_samples.append(sample)\n",
    "        \n",
    "        # Memory safeguard\n",
    "        current_ram = psutil.virtual_memory().used/(1024**3)\n",
    "        if current_ram > MAX_RAM_USAGE:\n",
    "            print(f\"Memory alert ({current_ram:.1f}GB). Reducing future samples.\")\n",
    "            target_per_file = int(target_per_file * 0.7)\n",
    "    \n",
    "    # --------------------------------------------------\n",
    "    # Phase 2: Combine and preprocess\n",
    "    # --------------------------------------------------\n",
    "    print(\"\\nCombining samples...\")\n",
    "    combined = pd.concat(all_samples, ignore_index=True)\n",
    "    del all_samples  # Free memory\n",
    "    \n",
    "    # Memory optimization\n",
    "    for col in [\"duration\", \"orig_bytes\", \"resp_bytes\"]:\n",
    "        if col in combined.columns:\n",
    "            combined[col] = pd.to_numeric(combined[col], downcast=\"float\")\n",
    "    \n",
    "    # Categorical encoding\n",
    "    for col in [\"proto\", \"service\", \"conn_state\"]:\n",
    "        if col in combined.columns:\n",
    "            combined[col] = combined[col].astype(\"category\").cat.codes\n",
    "    \n",
    "    # --------------------------------------------------\n",
    "    # Phase 3: Manual class balancing\n",
    "    # --------------------------------------------------\n",
    "    print(\"\\nBalancing classes without SMOTE...\")\n",
    "    \n",
    "    if \"label\" not in combined.columns:\n",
    "        raise KeyError(\"'label' column not found in the data\")\n",
    "    \n",
    "    # Get class distribution\n",
    "    class_counts = combined[\"label\"].value_counts()\n",
    "    min_class = class_counts.idxmin()\n",
    "    min_count = class_counts.min()\n",
    "    \n",
    "    balanced_dfs = []\n",
    "    \n",
    "    # Downsample majority classes\n",
    "    for class_name in class_counts.index:\n",
    "        class_df = combined[combined[\"label\"] == class_name]\n",
    "        if len(class_df) > min_count * 1.2:  # Only downsample if significantly larger\n",
    "            class_df = class_df.sample(min_count, random_state=42)\n",
    "        balanced_dfs.append(class_df)\n",
    "    \n",
    "    final_df = pd.concat(balanced_dfs, ignore_index=True)\n",
    "    \n",
    "    # --------------------------------------------------\n",
    "    # Phase 4: Final size adjustment\n",
    "    # --------------------------------------------------\n",
    "    # Calculate memory usage\n",
    "    final_size = final_df.memory_usage(deep=True).sum() / (1024**3)\n",
    "    \n",
    "    if final_size > TARGET_SIZE_GB * 1.1:\n",
    "        reduction = TARGET_SIZE_GB / final_size\n",
    "        final_df = final_df.sample(frac=reduction, random_state=42)\n",
    "        final_size = final_df.memory_usage(deep=True).sum() / (1024**3)\n",
    "    \n",
    "    print(f\"\\nFinal Dataset Stats:\")\n",
    "    print(f\"- Size: {final_size:.2f}GB\")\n",
    "    print(f\"- Samples: {len(final_df):,}\")\n",
    "    print(f\"- Class Distribution:\\n{final_df['label'].value_counts()}\")\n",
    "    \n",
    "    # Prepare for saving\n",
    "    X = final_df.drop(\"label\", axis=1)\n",
    "    y = final_df[\"label\"]\n",
    "    \n",
    "    # Save\n",
    "    dump({\n",
    "        \"features\": X,\n",
    "        \"labels\": y,\n",
    "        \"feature_names\": list(X.columns),\n",
    "        \"metadata\": {\n",
    "            \"source_files\": labeled_files,\n",
    "            \"creation_time\": pd.Timestamp.now(),\n",
    "            \"balance_method\": \"manual_downsampling\"\n",
    "        }\n",
    "    }, OUTPUT_FILE)\n",
    "    \n",
    "    # Performance report\n",
    "    total_time = (time.time() - start_time)/60\n",
    "    print(f\"\\nProcessing completed in {total_time:.1f} minutes\")\n",
    "    print(f\"Dataset saved to {OUTPUT_FILE}\")\n",
    "\n",
    "# =============================================\n",
    "# 4. Execute with error handling\n",
    "# =============================================\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        create_balanced_dataset()\n",
    "    except MemoryError:\n",
    "        print(\"\\nMemory Error! Try reducing target_per_file in config\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6f9794b-9b49-43c0-8751-cda6ff4d8a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting dataset creation without class balancing...\n",
      "Available RAM: 6.6GB\n",
      "[1/22] Sampling 1.labeled (0.1GB) -> 124143 rows...\n",
      "[2/22] Sampling 17conn.log.labeled (7.6GB) -> 240000 rows...\n",
      "[3/22] Sampling 20conn.log.labeled (0.0GB) -> 120011 rows...\n",
      "[4/22] Sampling 21.labeled (0.0GB) -> 120011 rows...\n",
      "[5/22] Sampling 3.labeled (0.0GB) -> 120681 rows...\n",
      "[6/22] Sampling 33conn.log.labeled (7.3GB) -> 240000 rows...\n",
      "[7/22] Sampling 34conn.log.labeled (0.0GB) -> 120084 rows...\n",
      "[8/22] Sampling 35conn.log.labeled (1.2GB) -> 157444 rows...\n",
      "[9/22] Sampling 36conn.log.labeled (1.7GB) -> 169952 rows...\n",
      "[10/22] Sampling 39conn.log.labeled (10.1GB) -> 240000 rows...\n",
      "[11/22] Sampling 42conn.log.labeled (0.0GB) -> 120016 rows...\n",
      "[12/22] Sampling 43conn.log.labeled (8.7GB) -> 240000 rows...\n",
      "[13/22] Sampling 44conn.log.labeled (0.0GB) -> 120000 rows...\n",
      "[14/22] Sampling 48conn.log.labeled (0.5GB) -> 134856 rows...\n",
      "[15/22] Sampling 49conn.log.labeled (0.8GB) -> 143850 rows...\n",
      "[16/22] Sampling 4conn.log.labeled (0.0GB) -> 120001 rows...\n",
      "[17/22] Sampling 52conn.log.labeled (2.9GB) -> 205783 rows...\n",
      "[18/22] Sampling 5conn.log.labeled (0.0GB) -> 120005 rows...\n",
      "[19/22] Sampling 60conn.log.labeled (0.4GB) -> 133063 rows...\n",
      "[20/22] Sampling 7conn.log.labeled (0.0GB) -> 120000 rows...\n",
      "[21/22] Sampling 8conn.log.labeled (0.0GB) -> 120039 rows...\n",
      "[22/22] Sampling 9conn.log.labeled (0.9GB) -> 147756 rows...\n",
      "\n",
      "Combining samples...\n",
      "\n",
      "Final Dataset Stats:\n",
      "- Size: 0.64GB\n",
      "- Samples: 1,968,771\n",
      "\n",
      "Error: 'label'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import psutil\n",
    "\n",
    "# =============================================\n",
    "# 1. Configuration\n",
    "# =============================================\n",
    "DATA_DIR = \".\"  # Directory with files\n",
    "OUTPUT_FILE = \"iot23_unbalanced.csv\"  # Output CSV filename\n",
    "TARGET_SIZE_GB = 1.8  # Target dataset size (1.5-2GB)\n",
    "MAX_RAM_USAGE = 9  # GB (2GB buffer for system)\n",
    "\n",
    "# Essential columns to keep (reduces memory)\n",
    "ESSENTIAL_COLS = [\n",
    "    \"ts\", \"uid\", \"id.orig_h\", \"id.orig_p\", \"id.resp_h\", \"id.resp_p\",\n",
    "    \"proto\", \"service\", \"duration\", \"orig_bytes\", \"resp_bytes\",\n",
    "    \"conn_state\", \"local_orig\", \"local_resp\", \"missed_bytes\",\n",
    "    \"history\", \"orig_pkts\", \"orig_ip_bytes\", \"resp_pkts\", \"resp_ip_bytes\",\n",
    "    \"label\"\n",
    "]\n",
    "\n",
    "# =============================================\n",
    "# 2. Memory-Optimized File Sampling\n",
    "# =============================================\n",
    "def sample_with_memory_control(filepath, target_rows):\n",
    "    \"\"\"Samples large files with proper handling of missing values\"\"\"\n",
    "    # First read the header to get column names\n",
    "    with open(filepath, 'r') as f:\n",
    "        for line in f:\n",
    "            if line.startswith('#fields'):\n",
    "                columns = line.strip().split('\\t')[1:]  # Get column names after #fields\n",
    "                break\n",
    "    \n",
    "    # Get approximate row count\n",
    "    with open(filepath, 'r') as f:\n",
    "        total_lines = sum(1 for line in f if not line.startswith('#'))\n",
    "    \n",
    "    # Dynamic chunksize (1% of file or 100k max)\n",
    "    chunksize = min(100000, max(50000, int(total_lines * 0.01)))\n",
    "    samples = []\n",
    "    \n",
    "    # Define NA values specific to Zeek logs\n",
    "    na_values = ['(empty)', '-', 'NA', 'N/A', 'nan', 'NaN', '']\n",
    "    \n",
    "    for chunk in pd.read_csv(\n",
    "        filepath,\n",
    "        sep=\"\\t\",\n",
    "        comment=\"#\",\n",
    "        chunksize=chunksize,\n",
    "        names=columns,\n",
    "        usecols=[col for col in ESSENTIAL_COLS if col in columns],\n",
    "        low_memory=False,  # Better for mixed types\n",
    "        na_values=na_values\n",
    "    ):\n",
    "        # Sample 5-15% from each chunk\n",
    "        sample_frac = min(0.15, max(0.05, target_rows/(total_lines * 2)))\n",
    "        samples.append(chunk.sample(frac=sample_frac, random_state=42))\n",
    "        \n",
    "        # Early termination\n",
    "        if sum(len(s) for s in samples) >= target_rows * 0.9:\n",
    "            break\n",
    "    \n",
    "    # Combine and return exact target size\n",
    "    combined = pd.concat(samples)\n",
    "    return combined.sample(min(target_rows, len(combined)), random_state=42)\n",
    "\n",
    "# =============================================\n",
    "# 3. Dataset Creation (Without Balancing)\n",
    "# =============================================\n",
    "def create_dataset():\n",
    "    print(\"Starting dataset creation without class balancing...\")\n",
    "    start_time = time.time()\n",
    "    mem = psutil.virtual_memory()\n",
    "    print(f\"Available RAM: {mem.available/(1024**3):.1f}GB\")\n",
    "    \n",
    "    # --------------------------------------------------\n",
    "    # Phase 1: Sample from all files with memory control\n",
    "    # --------------------------------------------------\n",
    "    all_samples = []\n",
    "    target_per_file = 120000  # Initial target rows per file\n",
    "    \n",
    "    labeled_files = [f for f in sorted(os.listdir(DATA_DIR)) if f.endswith(\".labeled\")]\n",
    "    if not labeled_files:\n",
    "        raise FileNotFoundError(\"No .labeled files found in directory\")\n",
    "    \n",
    "    for i, filename in enumerate(labeled_files):\n",
    "        filepath = os.path.join(DATA_DIR, filename)\n",
    "        file_size_gb = os.path.getsize(filepath)/(1024**3)\n",
    "        \n",
    "        # Adjust sample size based on file size\n",
    "        file_target = int(target_per_file * min(2, 1 + file_size_gb/4))\n",
    "        \n",
    "        print(f\"[{i+1}/{len(labeled_files)}] Sampling {filename} ({file_size_gb:.1f}GB) -> {file_target} rows...\")\n",
    "        sample = sample_with_memory_control(filepath, file_target)\n",
    "        all_samples.append(sample)\n",
    "        \n",
    "        # Memory safeguard\n",
    "        current_ram = psutil.virtual_memory().used/(1024**3)\n",
    "        if current_ram > MAX_RAM_USAGE:\n",
    "            print(f\"Memory alert ({current_ram:.1f}GB). Reducing future samples.\")\n",
    "            target_per_file = int(target_per_file * 0.7)\n",
    "    \n",
    "    # --------------------------------------------------\n",
    "    # Phase 2: Combine and preprocess\n",
    "    # --------------------------------------------------\n",
    "    print(\"\\nCombining samples...\")\n",
    "    combined = pd.concat(all_samples, ignore_index=True)\n",
    "    del all_samples  # Free memory\n",
    "    \n",
    "    # Handle missing values in numeric columns\n",
    "    numeric_cols = [\"duration\", \"orig_bytes\", \"resp_bytes\", \"orig_pkts\", \"resp_pkts\", \n",
    "                   \"orig_ip_bytes\", \"resp_ip_bytes\", \"missed_bytes\"]\n",
    "    for col in numeric_cols:\n",
    "        if col in combined.columns:\n",
    "            combined[col] = pd.to_numeric(combined[col], errors='coerce').fillna(0)\n",
    "    \n",
    "    # Categorical encoding\n",
    "    for col in [\"proto\", \"service\", \"conn_state\"]:\n",
    "        if col in combined.columns:\n",
    "            combined[col] = combined[col].astype(\"category\").cat.codes\n",
    "    \n",
    "    # --------------------------------------------------\n",
    "    # Phase 3: Final size adjustment\n",
    "    # --------------------------------------------------\n",
    "    # Calculate memory usage\n",
    "    final_size = combined.memory_usage(deep=True).sum() / (1024**3)\n",
    "    \n",
    "    if final_size > TARGET_SIZE_GB * 1.1:\n",
    "        reduction = TARGET_SIZE_GB / final_size\n",
    "        combined = combined.sample(frac=reduction, random_state=42)\n",
    "        final_size = combined.memory_usage(deep=True).sum() / (1024**3)\n",
    "    \n",
    "    print(f\"\\nFinal Dataset Stats:\")\n",
    "    print(f\"- Size: {final_size:.2f}GB\")\n",
    "    print(f\"- Samples: {len(combined):,}\")\n",
    "    print(f\"- Class Distribution:\\n{combined['label'].value_counts()}\")\n",
    "    \n",
    "    # Save as CSV in chunks to avoid memory issues\n",
    "    print(\"\\nSaving as CSV...\")\n",
    "    chunksize = 100000  # Rows per chunk\n",
    "    for i in range(0, len(combined), chunksize):\n",
    "        chunk = combined.iloc[i:i+chunksize]\n",
    "        mode = 'w' if i == 0 else 'a'\n",
    "        header = i == 0\n",
    "        chunk.to_csv(OUTPUT_FILE, mode=mode, header=header, index=False)\n",
    "        print(f\"Saved rows {i} to {min(i+chunksize, len(combined))}\")\n",
    "    \n",
    "    # Performance report\n",
    "    total_time = (time.time() - start_time)/60\n",
    "    print(f\"\\nProcessing completed in {total_time:.1f} minutes\")\n",
    "    print(f\"Dataset saved to {OUTPUT_FILE}\")\n",
    "\n",
    "# =============================================\n",
    "# 4. Execute with error handling\n",
    "# =============================================\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        create_dataset()\n",
    "    except MemoryError:\n",
    "        print(\"\\nMemory Error! Try reducing target_per_file in config\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2cdba1ea-381a-4b95-a791-7f5d93fd16d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting dataset creation without class balancing...\n",
      "Available RAM: 6.4GB\n",
      "[1/22] Sampling 1.labeled (0.1GB) -> 124143 rows...\n",
      "[2/22] Sampling 17conn.log.labeled (7.6GB) -> 240000 rows...\n",
      "[3/22] Sampling 20conn.log.labeled (0.0GB) -> 120011 rows...\n",
      "[4/22] Sampling 21.labeled (0.0GB) -> 120011 rows...\n",
      "[5/22] Sampling 3.labeled (0.0GB) -> 120681 rows...\n",
      "[6/22] Sampling 33conn.log.labeled (7.3GB) -> 240000 rows...\n",
      "[7/22] Sampling 34conn.log.labeled (0.0GB) -> 120084 rows...\n",
      "[8/22] Sampling 35conn.log.labeled (1.2GB) -> 157444 rows...\n",
      "[9/22] Sampling 36conn.log.labeled (1.7GB) -> 169952 rows...\n",
      "[10/22] Sampling 39conn.log.labeled (10.1GB) -> 240000 rows...\n",
      "[11/22] Sampling 42conn.log.labeled (0.0GB) -> 120016 rows...\n",
      "[12/22] Sampling 43conn.log.labeled (8.7GB) -> 240000 rows...\n",
      "[13/22] Sampling 44conn.log.labeled (0.0GB) -> 120000 rows...\n",
      "[14/22] Sampling 48conn.log.labeled (0.5GB) -> 134856 rows...\n",
      "[15/22] Sampling 49conn.log.labeled (0.8GB) -> 143850 rows...\n",
      "[16/22] Sampling 4conn.log.labeled (0.0GB) -> 120001 rows...\n",
      "[17/22] Sampling 52conn.log.labeled (2.9GB) -> 205783 rows...\n",
      "[18/22] Sampling 5conn.log.labeled (0.0GB) -> 120005 rows...\n",
      "[19/22] Sampling 60conn.log.labeled (0.4GB) -> 133063 rows...\n",
      "[20/22] Sampling 7conn.log.labeled (0.0GB) -> 120000 rows...\n",
      "[21/22] Sampling 8conn.log.labeled (0.0GB) -> 120039 rows...\n",
      "[22/22] Sampling 9conn.log.labeled (0.9GB) -> 147756 rows...\n",
      "\n",
      "Combining samples...\n",
      "\n",
      "Final Dataset Stats:\n",
      "- Size: 0.61GB\n",
      "- Samples: 1,968,771\n",
      "- No label column found in the data\n",
      "\n",
      "Saving as CSV...\n",
      "Saved rows 0 to 100000\n",
      "Saved rows 100000 to 200000\n",
      "Saved rows 200000 to 300000\n",
      "Saved rows 300000 to 400000\n",
      "Saved rows 400000 to 500000\n",
      "Saved rows 500000 to 600000\n",
      "Saved rows 600000 to 700000\n",
      "Saved rows 700000 to 800000\n",
      "Saved rows 800000 to 900000\n",
      "Saved rows 900000 to 1000000\n",
      "Saved rows 1000000 to 1100000\n",
      "Saved rows 1100000 to 1200000\n",
      "Saved rows 1200000 to 1300000\n",
      "Saved rows 1300000 to 1400000\n",
      "Saved rows 1400000 to 1500000\n",
      "Saved rows 1500000 to 1600000\n",
      "Saved rows 1600000 to 1700000\n",
      "Saved rows 1700000 to 1800000\n",
      "Saved rows 1800000 to 1900000\n",
      "Saved rows 1900000 to 1968771\n",
      "\n",
      "Processing completed in 6.9 minutes\n",
      "Dataset saved to iot23_unbalanced.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import psutil\n",
    "\n",
    "# =============================================\n",
    "# 1. Configuration\n",
    "# =============================================\n",
    "DATA_DIR = \".\"  # Directory with files\n",
    "OUTPUT_FILE = \"iot23_unbalanced.csv\"  # Output CSV filename\n",
    "TARGET_SIZE_GB = 1.8  # Target dataset size (1.5-2GB)\n",
    "MAX_RAM_USAGE = 9  # GB (2GB buffer for system)\n",
    "\n",
    "# Essential columns to keep (reduces memory)\n",
    "ESSENTIAL_COLS = [\n",
    "    \"ts\", \"uid\", \"id.orig_h\", \"id.orig_p\", \"id.resp_h\", \"id.resp_p\",\n",
    "    \"proto\", \"service\", \"duration\", \"orig_bytes\", \"resp_bytes\",\n",
    "    \"conn_state\", \"missed_bytes\", \"history\", \"orig_pkts\",\n",
    "    \"orig_ip_bytes\", \"resp_pkts\", \"resp_ip_bytes\", \"ip_proto\"\n",
    "    # Removed 'label' since it's not found in your data\n",
    "]\n",
    "\n",
    "# =============================================\n",
    "# 2. Memory-Optimized File Sampling\n",
    "# =============================================\n",
    "def sample_with_memory_control(filepath, target_rows):\n",
    "    \"\"\"Samples large files with proper handling of missing values\"\"\"\n",
    "    # First read the header to get column names\n",
    "    with open(filepath, 'r') as f:\n",
    "        for line in f:\n",
    "            if line.startswith('#fields'):\n",
    "                columns = line.strip().split('\\t')[1:]  # Get column names after #fields\n",
    "                break\n",
    "    \n",
    "    # Get approximate row count\n",
    "    with open(filepath, 'r') as f:\n",
    "        total_lines = sum(1 for line in f if not line.startswith('#'))\n",
    "    \n",
    "    # Dynamic chunksize (1% of file or 100k max)\n",
    "    chunksize = min(100000, max(50000, int(total_lines * 0.01)))\n",
    "    samples = []\n",
    "    \n",
    "    # Define NA values specific to Zeek logs\n",
    "    na_values = ['(empty)', '-', 'NA', 'N/A', 'nan', 'NaN', '']\n",
    "    \n",
    "    # Only include columns that exist in the file\n",
    "    usecols = [col for col in ESSENTIAL_COLS if col in columns]\n",
    "    \n",
    "    for chunk in pd.read_csv(\n",
    "        filepath,\n",
    "        sep=\"\\t\",\n",
    "        comment=\"#\",\n",
    "        chunksize=chunksize,\n",
    "        names=columns,\n",
    "        usecols=usecols,\n",
    "        low_memory=False,  # Better for mixed types\n",
    "        na_values=na_values\n",
    "    ):\n",
    "        # Sample 5-15% from each chunk\n",
    "        sample_frac = min(0.15, max(0.05, target_rows/(total_lines * 2)))\n",
    "        samples.append(chunk.sample(frac=sample_frac, random_state=42))\n",
    "        \n",
    "        # Early termination\n",
    "        if sum(len(s) for s in samples) >= target_rows * 0.9:\n",
    "            break\n",
    "    \n",
    "    # Combine and return exact target size\n",
    "    combined = pd.concat(samples)\n",
    "    return combined.sample(min(target_rows, len(combined)), random_state=42)\n",
    "\n",
    "# =============================================\n",
    "# 3. Dataset Creation (Without Balancing)\n",
    "# =============================================\n",
    "def create_dataset():\n",
    "    print(\"Starting dataset creation without class balancing...\")\n",
    "    start_time = time.time()\n",
    "    mem = psutil.virtual_memory()\n",
    "    print(f\"Available RAM: {mem.available/(1024**3):.1f}GB\")\n",
    "    \n",
    "    # --------------------------------------------------\n",
    "    # Phase 1: Sample from all files with memory control\n",
    "    # --------------------------------------------------\n",
    "    all_samples = []\n",
    "    target_per_file = 120000  # Initial target rows per file\n",
    "    \n",
    "    labeled_files = [f for f in sorted(os.listdir(DATA_DIR)) if f.endswith(\".labeled\")]\n",
    "    if not labeled_files:\n",
    "        raise FileNotFoundError(\"No .labeled files found in directory\")\n",
    "    \n",
    "    for i, filename in enumerate(labeled_files):\n",
    "        filepath = os.path.join(DATA_DIR, filename)\n",
    "        file_size_gb = os.path.getsize(filepath)/(1024**3)\n",
    "        \n",
    "        # Adjust sample size based on file size\n",
    "        file_target = int(target_per_file * min(2, 1 + file_size_gb/4))\n",
    "        \n",
    "        print(f\"[{i+1}/{len(labeled_files)}] Sampling {filename} ({file_size_gb:.1f}GB) -> {file_target} rows...\")\n",
    "        sample = sample_with_memory_control(filepath, file_target)\n",
    "        all_samples.append(sample)\n",
    "        \n",
    "        # Memory safeguard\n",
    "        current_ram = psutil.virtual_memory().used/(1024**3)\n",
    "        if current_ram > MAX_RAM_USAGE:\n",
    "            print(f\"Memory alert ({current_ram:.1f}GB). Reducing future samples.\")\n",
    "            target_per_file = int(target_per_file * 0.7)\n",
    "    \n",
    "    # --------------------------------------------------\n",
    "    # Phase 2: Combine and preprocess\n",
    "    # --------------------------------------------------\n",
    "    print(\"\\nCombining samples...\")\n",
    "    combined = pd.concat(all_samples, ignore_index=True)\n",
    "    del all_samples  # Free memory\n",
    "    \n",
    "    # Handle missing values in numeric columns\n",
    "    numeric_cols = [\"duration\", \"orig_bytes\", \"resp_bytes\", \"orig_pkts\", \"resp_pkts\", \n",
    "                   \"orig_ip_bytes\", \"resp_ip_bytes\", \"missed_bytes\"]\n",
    "    for col in numeric_cols:\n",
    "        if col in combined.columns:\n",
    "            combined[col] = pd.to_numeric(combined[col], errors='coerce').fillna(0)\n",
    "    \n",
    "    # Categorical encoding\n",
    "    for col in [\"proto\", \"service\", \"conn_state\"]:\n",
    "        if col in combined.columns:\n",
    "            combined[col] = combined[col].astype(\"category\").cat.codes\n",
    "    \n",
    "    # --------------------------------------------------\n",
    "    # Phase 3: Final size adjustment\n",
    "    # --------------------------------------------------\n",
    "    # Calculate memory usage\n",
    "    final_size = combined.memory_usage(deep=True).sum() / (1024**3)\n",
    "    \n",
    "    if final_size > TARGET_SIZE_GB * 1.1:\n",
    "        reduction = TARGET_SIZE_GB / final_size\n",
    "        combined = combined.sample(frac=reduction, random_state=42)\n",
    "        final_size = combined.memory_usage(deep=True).sum() / (1024**3)\n",
    "    \n",
    "    print(f\"\\nFinal Dataset Stats:\")\n",
    "    print(f\"- Size: {final_size:.2f}GB\")\n",
    "    print(f\"- Samples: {len(combined):,}\")\n",
    "    \n",
    "    # Check if label column exists before trying to print distribution\n",
    "    if 'label' in combined.columns:\n",
    "        print(f\"- Class Distribution:\\n{combined['label'].value_counts()}\")\n",
    "    else:\n",
    "        print(\"- No label column found in the data\")\n",
    "    \n",
    "    # Save as CSV in chunks to avoid memory issues\n",
    "    print(\"\\nSaving as CSV...\")\n",
    "    chunksize = 100000  # Rows per chunk\n",
    "    for i in range(0, len(combined), chunksize):\n",
    "        chunk = combined.iloc[i:i+chunksize]\n",
    "        mode = 'w' if i == 0 else 'a'\n",
    "        header = i == 0\n",
    "        chunk.to_csv(OUTPUT_FILE, mode=mode, header=header, index=False)\n",
    "        print(f\"Saved rows {i} to {min(i+chunksize, len(combined))}\")\n",
    "    \n",
    "    # Performance report\n",
    "    total_time = (time.time() - start_time)/60\n",
    "    print(f\"\\nProcessing completed in {total_time:.1f} minutes\")\n",
    "    print(f\"Dataset saved to {OUTPUT_FILE}\")\n",
    "\n",
    "# =============================================\n",
    "# 4. Execute with error handling\n",
    "# =============================================\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        create_dataset()\n",
    "    except MemoryError:\n",
    "        print(\"\\nMemory Error! Try reducing target_per_file in config\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ab0edb3-1b04-449b-837b-339a094ece15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1.labeled...\n",
      "Processing 20conn.log.labeled...\n",
      "Processing 21.labeled...\n",
      "Processing 3.labeled...\n",
      "Processing 4conn.log.labeled...\n",
      "Processing 5conn.log.labeled...\n",
      "Processing 7conn.log.labeled...\n",
      "Processing 8conn.log.labeled...\n",
      "Processing 9conn.log.labeled...\n",
      "Label distribution:\n",
      "label\n",
      "unknown    7561998\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Saved processed data to network_training_data.csv\n",
      "Final shape: (7561998, 12)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Configuration\n",
    "DATA_DIR = \".\"  # Directory containing your conn.log.labeled files\n",
    "OUTPUT_CSV = \"network_training_data.csv\"  # Output file name\n",
    "\n",
    "# =============================================\n",
    "# 1. Load and Parse Zeek Logs\n",
    "# =============================================\n",
    "def parse_zeek_log(filepath):\n",
    "    \"\"\"Parse a single Zeek conn.log.labeled file\"\"\"\n",
    "    # Read the header to get column names\n",
    "    with open(filepath, 'r') as f:\n",
    "        for line in f:\n",
    "            if line.startswith('#fields'):\n",
    "                columns = line.strip().split('\\t')[1:]\n",
    "                break\n",
    "    \n",
    "    # Load data with proper handling of Zeek-specific values\n",
    "    df = pd.read_csv(\n",
    "        filepath,\n",
    "        sep='\\t',\n",
    "        comment='#',\n",
    "        names=columns,\n",
    "        na_values=['(empty)', '-', 'NA'],\n",
    "        low_memory=False\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "# =============================================\n",
    "# 2. Feature Engineering\n",
    "# =============================================\n",
    "def engineer_features(df):\n",
    "    \"\"\"Create ML-ready features from raw Zeek data\"\"\"\n",
    "    \n",
    "    # Basic cleaning\n",
    "    df = df.dropna(subset=['id.orig_h', 'id.resp_h'])  # Remove rows missing IPs\n",
    "    \n",
    "    # Protocol handling\n",
    "    proto_map = {0: 'unknown', 1: 'tcp', 2: 'udp', 3: 'icmp'}\n",
    "    df['proto'] = df['proto'].map(proto_map).fillna('other')\n",
    "    \n",
    "    # Connection state encoding\n",
    "    state_map = {\n",
    "        'S0': 'attempt', 'S1': 'established', 'S2': 'attempt',\n",
    "        'S3': 'attempt', 'SF': 'normal', 'REJ': 'rejected',\n",
    "        'RSTO': 'reset', 'RSTR': 'reset', 'RSTOS0': 'reset',\n",
    "        'RSTRH': 'reset', 'SH': 'scan', 'SHR': 'scan',\n",
    "        'OTH': 'other'\n",
    "    }\n",
    "    df['conn_state'] = df['conn_state'].map(state_map).fillna('other')\n",
    "    \n",
    "    # Service inference from ports\n",
    "    def infer_service(row):\n",
    "        port = row['id.resp_p']\n",
    "        if row['proto'] == 'tcp':\n",
    "            if port == 80 or port == 443: return 'http'\n",
    "            elif port == 22: return 'ssh'\n",
    "            elif port == 21: return 'ftp'\n",
    "        elif row['proto'] == 'udp' and port == 53: return 'dns'\n",
    "        return 'other'\n",
    "    \n",
    "    df['service'] = df.apply(infer_service, axis=1)\n",
    "    \n",
    "    # Numeric features\n",
    "    df['duration'] = df['duration'].fillna(0)\n",
    "    df['bytes_ratio'] = (df['orig_bytes'] + 1) / (df['resp_bytes'] + 1)\n",
    "    df['pkts_ratio'] = (df['orig_pkts'] + 1) / (df['resp_pkts'] + 1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# =============================================\n",
    "# 3. Label Processing\n",
    "# =============================================\n",
    "def process_labels(df):\n",
    "    \"\"\"Extract and encode labels from Zeek's annotations\"\"\"\n",
    "    \n",
    "    # Zeek typically adds labels like:\n",
    "    # \"label\": \"Malware\", \"Botnet\", \"Scan\", etc.\n",
    "    if 'label' not in df.columns:\n",
    "        df['label'] = 'unknown'  # Default if no labels exist\n",
    "    else:\n",
    "        # Clean label strings\n",
    "        df['label'] = df['label'].str.extract(r'\\\"(.*?)\\\"')[0].fillna('benign')\n",
    "    \n",
    "    # Encode labels\n",
    "    le = LabelEncoder()\n",
    "    df['label_encoded'] = le.fit_transform(df['label'])\n",
    "    \n",
    "    print(\"Label distribution:\")\n",
    "    print(df['label'].value_counts())\n",
    "    \n",
    "    return df, le\n",
    "\n",
    "# =============================================\n",
    "# 4. Main Processing Function\n",
    "# =============================================\n",
    "def process_all_files():\n",
    "    all_data = []\n",
    "    \n",
    "    # Process each file\n",
    "    for filename in os.listdir(DATA_DIR):\n",
    "        if filename.endswith('.labeled'):\n",
    "            print(f\"Processing {filename}...\")\n",
    "            filepath = os.path.join(DATA_DIR, filename)\n",
    "            df = parse_zeek_log(filepath)\n",
    "            df = engineer_features(df)\n",
    "            all_data.append(df)\n",
    "    \n",
    "    # Combine all data\n",
    "    combined = pd.concat(all_data, ignore_index=True)\n",
    "    \n",
    "    # Process labels\n",
    "    combined, label_encoder = process_labels(combined)\n",
    "    \n",
    "    # Select final features\n",
    "    features = [\n",
    "        'duration', 'orig_bytes', 'resp_bytes', 'bytes_ratio', 'pkts_ratio',\n",
    "        'orig_pkts', 'resp_pkts', 'proto', 'service', 'conn_state',\n",
    "        'label', 'label_encoded'\n",
    "    ]\n",
    "    features = [f for f in features if f in combined.columns]\n",
    "    \n",
    "    final_df = combined[features]\n",
    "    \n",
    "    # Save to CSV\n",
    "    final_df.to_csv(OUTPUT_CSV, index=False)\n",
    "    print(f\"\\nSaved processed data to {OUTPUT_CSV}\")\n",
    "    print(f\"Final shape: {final_df.shape}\")\n",
    "    \n",
    "    return final_df, label_encoder\n",
    "\n",
    "# Run the processing\n",
    "if __name__ == \"__main__\":\n",
    "    df, le = process_all_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e39cff00-2fac-401b-a308-c716c017cc14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('1532510451.648888\\tCnR6zKxrWbFw26ua7\\t192.168.100.111\\t40008\\t46.28.110.244\\t123\\tudp\\t-\\t0.004751\\t48\\t48\\tSF\\t-\\t-\\t0\\tDd\\t1\\t76\\t1\\t76\\t(empty)   Benign   -',)\n",
      " ('1532511003.118878\\tCcfURS3zgEMzX0RqTc\\t192.168.100.102\\t57849\\t192.168.100.111\\t22\\ttcp\\t-\\t108.421563\\t2376\\t96\\tOTH\\t-\\t-\\t0\\tDAd\\t5\\t332\\t2\\t200\\t(empty)   Benign   -',)\n",
      " ('1532511739.837566\\tCPhAqD1EhNOoPVA6ha\\t192.168.100.102\\t59670\\t192.168.100.111\\t22\\ttcp\\t-\\t0.001244\\t21\\t0\\tS0\\t-\\t-\\t0\\tSAD\\t3\\t189\\t0\\t0\\t(empty)   Benign   -',)\n",
      " ...\n",
      " ('1532591674.249972\\tCxHWyM2vp5x1mCZmp3\\t192.168.100.111\\t39234\\t192.121.45.63\\t23\\ttcp\\t-\\t165.760290\\t0\\t0\\tS1\\t-\\t-\\t0\\tShA\\t5\\t220\\t1\\t44\\t(empty)   Benign   -',)\n",
      " ('1532595145.171589\\tCtmaRjXW7FOus0sPa\\t168.102.14.4\\t11\\t192.168.100.111\\t0\\ticmp\\t-\\t-\\t-\\t-\\tOTH\\t-\\t-\\t0\\t-\\t1\\t68\\t0\\t0\\t(empty)   Benign   -',)\n",
      " ('1532595149.229773\\tCUVfp02K1YEXmtDf2h\\t192.94.118.248\\t11\\t192.168.100.111\\t0\\ticmp\\t-\\t-\\t-\\t-\\tOTH\\t-\\t-\\t0\\t-\\t1\\t56\\t0\\t0\\t(empty)   Benign   -',)]\n",
      "fields string exists in file\n",
      "Line Number: 6\n",
      "Line: #fields\tts\tuid\tid.orig_h\tid.orig_p\tid.resp_h\tid.resp_p\tproto\tservice\tduration\torig_bytes\tresp_bytes\tconn_state\tlocal_orig\tlocal_resp\tmissed_bytes\thistory\torig_pkts\torig_ip_bytes\tresp_pkts\tresp_ip_bytes\ttunnel_parents   label   detailed-label\n",
      "\n",
      "                        0                   1                2      3   \\\n",
      "0        1532510451.648888   CnR6zKxrWbFw26ua7  192.168.100.111  40008   \n",
      "1        1532511003.118878  CcfURS3zgEMzX0RqTc  192.168.100.102  57849   \n",
      "2        1532511739.837566  CPhAqD1EhNOoPVA6ha  192.168.100.102  59670   \n",
      "3        1532512077.774888   CFr56iYeYk4YZXrPf  192.168.100.102  59701   \n",
      "4        1532512335.605565  C8a9xW23hF8nDgeNtj  192.168.100.111  36199   \n",
      "...                    ...                 ...              ...    ...   \n",
      "6378288  1532595195.007148  CIHonx2ymqlX5DqnU2  192.168.100.111  28057   \n",
      "6378289  1532594748.215022  CwhtUD3BIdOP5L6Y8g  192.168.100.111  52876   \n",
      "6378290  1532591674.249972  CxHWyM2vp5x1mCZmp3  192.168.100.111  39234   \n",
      "6378291  1532595145.171589   CtmaRjXW7FOus0sPa     168.102.14.4     11   \n",
      "6378292  1532595149.229773  CUVfp02K1YEXmtDf2h   192.94.118.248     11   \n",
      "\n",
      "                      4    5     6    7           8     9   ...   11 12 13 14  \\\n",
      "0          46.28.110.244  123   udp    -    0.004751    48  ...   SF  -  -  0   \n",
      "1        192.168.100.111   22   tcp    -  108.421563  2376  ...  OTH  -  -  0   \n",
      "2        192.168.100.111   22   tcp    -    0.001244    21  ...   S0  -  -  0   \n",
      "3        192.168.100.111   22   tcp    -    0.000738    21  ...   S0  -  -  0   \n",
      "4          192.168.100.1   53   udp  dns           -     -  ...   S0  -  -  0   \n",
      "...                  ...  ...   ...  ...         ...   ...  ...  ... .. .. ..   \n",
      "6378288    173.94.58.125   23   tcp    -           -     -  ...   S0  -  -  0   \n",
      "6378289    69.196.96.231   23   tcp    -  209.874367     0  ...   S1  -  -  0   \n",
      "6378290    192.121.45.63   23   tcp    -  165.760290     0  ...   S1  -  -  0   \n",
      "6378291  192.168.100.111    0  icmp    -           -     -  ...  OTH  -  -  0   \n",
      "6378292  192.168.100.111    0  icmp    -           -     -  ...  OTH  -  -  0   \n",
      "\n",
      "          15 16   17 18   19                                               20  \n",
      "0         Dd  1   76  1   76                             (empty)   Benign   -  \n",
      "1        DAd  5  332  2  200                             (empty)   Benign   -  \n",
      "2        SAD  3  189  0    0                             (empty)   Benign   -  \n",
      "3        SAD  3  189  0    0                             (empty)   Benign   -  \n",
      "4          D  1   58  0    0                             (empty)   Benign   -  \n",
      "...      ... ..  ... ..  ...                                              ...  \n",
      "6378288    S  1   40  0    0  (empty)   Malicious   PartOfAHorizontalPortScan  \n",
      "6378289  ShA  8  360  1   44                             (empty)   Benign   -  \n",
      "6378290  ShA  5  220  1   44                             (empty)   Benign   -  \n",
      "6378291    -  1   68  0    0                             (empty)   Benign   -  \n",
      "6378292    -  1   56  0    0                             (empty)   Benign   -  \n",
      "\n",
      "[6378293 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Assign the filename\n",
    "fpath = r\"9conn.log.labeled\"\n",
    "# Import file using np.recfromcsv: d\n",
    "narray = np.recfromcsv(fpath, encoding=None)\n",
    "\n",
    "# Print out first n records of d \n",
    "print(narray)\n",
    "# string to search in file\n",
    "word = 'fields'\n",
    "with open(fpath, 'r') as fp:\n",
    "    # read all lines in a list\n",
    "    lines = fp.readlines()\n",
    "    for line in lines:\n",
    "        # check if string present on a current line\n",
    "        if line.find(word) != -1:\n",
    "            print(word, 'string exists in file')\n",
    "            print('Line Number:', lines.index(line))\n",
    "            print('Line:', line)\n",
    "            linefields = line\n",
    "x = linefields.split(\"\\t\")\n",
    "df = pd.concat([pd.Series(l.astype(str).split('\\t')) for l in narray], axis=1).T\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30109e31-3003-43d0-bf32-8004974f8276",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1532510451.648888</td>\n",
       "      <td>CnR6zKxrWbFw26ua7</td>\n",
       "      <td>192.168.100.111</td>\n",
       "      <td>40008</td>\n",
       "      <td>46.28.110.244</td>\n",
       "      <td>123</td>\n",
       "      <td>udp</td>\n",
       "      <td>-</td>\n",
       "      <td>0.004751</td>\n",
       "      <td>48</td>\n",
       "      <td>...</td>\n",
       "      <td>SF</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>Dd</td>\n",
       "      <td>1</td>\n",
       "      <td>76</td>\n",
       "      <td>1</td>\n",
       "      <td>76</td>\n",
       "      <td>(empty)   Benign   -</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1532511003.118878</td>\n",
       "      <td>CcfURS3zgEMzX0RqTc</td>\n",
       "      <td>192.168.100.102</td>\n",
       "      <td>57849</td>\n",
       "      <td>192.168.100.111</td>\n",
       "      <td>22</td>\n",
       "      <td>tcp</td>\n",
       "      <td>-</td>\n",
       "      <td>108.421563</td>\n",
       "      <td>2376</td>\n",
       "      <td>...</td>\n",
       "      <td>OTH</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>DAd</td>\n",
       "      <td>5</td>\n",
       "      <td>332</td>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>(empty)   Benign   -</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1532511739.837566</td>\n",
       "      <td>CPhAqD1EhNOoPVA6ha</td>\n",
       "      <td>192.168.100.102</td>\n",
       "      <td>59670</td>\n",
       "      <td>192.168.100.111</td>\n",
       "      <td>22</td>\n",
       "      <td>tcp</td>\n",
       "      <td>-</td>\n",
       "      <td>0.001244</td>\n",
       "      <td>21</td>\n",
       "      <td>...</td>\n",
       "      <td>S0</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>SAD</td>\n",
       "      <td>3</td>\n",
       "      <td>189</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(empty)   Benign   -</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1532512077.774888</td>\n",
       "      <td>CFr56iYeYk4YZXrPf</td>\n",
       "      <td>192.168.100.102</td>\n",
       "      <td>59701</td>\n",
       "      <td>192.168.100.111</td>\n",
       "      <td>22</td>\n",
       "      <td>tcp</td>\n",
       "      <td>-</td>\n",
       "      <td>0.000738</td>\n",
       "      <td>21</td>\n",
       "      <td>...</td>\n",
       "      <td>S0</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>SAD</td>\n",
       "      <td>3</td>\n",
       "      <td>189</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(empty)   Benign   -</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1532512335.605565</td>\n",
       "      <td>C8a9xW23hF8nDgeNtj</td>\n",
       "      <td>192.168.100.111</td>\n",
       "      <td>36199</td>\n",
       "      <td>192.168.100.1</td>\n",
       "      <td>53</td>\n",
       "      <td>udp</td>\n",
       "      <td>dns</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>...</td>\n",
       "      <td>S0</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>D</td>\n",
       "      <td>1</td>\n",
       "      <td>58</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(empty)   Benign   -</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6378288</th>\n",
       "      <td>1532595195.007148</td>\n",
       "      <td>CIHonx2ymqlX5DqnU2</td>\n",
       "      <td>192.168.100.111</td>\n",
       "      <td>28057</td>\n",
       "      <td>173.94.58.125</td>\n",
       "      <td>23</td>\n",
       "      <td>tcp</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>...</td>\n",
       "      <td>S0</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(empty)   Malicious   PartOfAHorizontalPortScan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6378289</th>\n",
       "      <td>1532594748.215022</td>\n",
       "      <td>CwhtUD3BIdOP5L6Y8g</td>\n",
       "      <td>192.168.100.111</td>\n",
       "      <td>52876</td>\n",
       "      <td>69.196.96.231</td>\n",
       "      <td>23</td>\n",
       "      <td>tcp</td>\n",
       "      <td>-</td>\n",
       "      <td>209.874367</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>S1</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>ShA</td>\n",
       "      <td>8</td>\n",
       "      <td>360</td>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>(empty)   Benign   -</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6378290</th>\n",
       "      <td>1532591674.249972</td>\n",
       "      <td>CxHWyM2vp5x1mCZmp3</td>\n",
       "      <td>192.168.100.111</td>\n",
       "      <td>39234</td>\n",
       "      <td>192.121.45.63</td>\n",
       "      <td>23</td>\n",
       "      <td>tcp</td>\n",
       "      <td>-</td>\n",
       "      <td>165.760290</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>S1</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>ShA</td>\n",
       "      <td>5</td>\n",
       "      <td>220</td>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>(empty)   Benign   -</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6378291</th>\n",
       "      <td>1532595145.171589</td>\n",
       "      <td>CtmaRjXW7FOus0sPa</td>\n",
       "      <td>168.102.14.4</td>\n",
       "      <td>11</td>\n",
       "      <td>192.168.100.111</td>\n",
       "      <td>0</td>\n",
       "      <td>icmp</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>...</td>\n",
       "      <td>OTH</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>1</td>\n",
       "      <td>68</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(empty)   Benign   -</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6378292</th>\n",
       "      <td>1532595149.229773</td>\n",
       "      <td>CUVfp02K1YEXmtDf2h</td>\n",
       "      <td>192.94.118.248</td>\n",
       "      <td>11</td>\n",
       "      <td>192.168.100.111</td>\n",
       "      <td>0</td>\n",
       "      <td>icmp</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>...</td>\n",
       "      <td>OTH</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>1</td>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(empty)   Benign   -</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6378293 rows  21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        0                   1                2      3   \\\n",
       "0        1532510451.648888   CnR6zKxrWbFw26ua7  192.168.100.111  40008   \n",
       "1        1532511003.118878  CcfURS3zgEMzX0RqTc  192.168.100.102  57849   \n",
       "2        1532511739.837566  CPhAqD1EhNOoPVA6ha  192.168.100.102  59670   \n",
       "3        1532512077.774888   CFr56iYeYk4YZXrPf  192.168.100.102  59701   \n",
       "4        1532512335.605565  C8a9xW23hF8nDgeNtj  192.168.100.111  36199   \n",
       "...                    ...                 ...              ...    ...   \n",
       "6378288  1532595195.007148  CIHonx2ymqlX5DqnU2  192.168.100.111  28057   \n",
       "6378289  1532594748.215022  CwhtUD3BIdOP5L6Y8g  192.168.100.111  52876   \n",
       "6378290  1532591674.249972  CxHWyM2vp5x1mCZmp3  192.168.100.111  39234   \n",
       "6378291  1532595145.171589   CtmaRjXW7FOus0sPa     168.102.14.4     11   \n",
       "6378292  1532595149.229773  CUVfp02K1YEXmtDf2h   192.94.118.248     11   \n",
       "\n",
       "                      4    5     6    7           8     9   ...   11 12 13 14  \\\n",
       "0          46.28.110.244  123   udp    -    0.004751    48  ...   SF  -  -  0   \n",
       "1        192.168.100.111   22   tcp    -  108.421563  2376  ...  OTH  -  -  0   \n",
       "2        192.168.100.111   22   tcp    -    0.001244    21  ...   S0  -  -  0   \n",
       "3        192.168.100.111   22   tcp    -    0.000738    21  ...   S0  -  -  0   \n",
       "4          192.168.100.1   53   udp  dns           -     -  ...   S0  -  -  0   \n",
       "...                  ...  ...   ...  ...         ...   ...  ...  ... .. .. ..   \n",
       "6378288    173.94.58.125   23   tcp    -           -     -  ...   S0  -  -  0   \n",
       "6378289    69.196.96.231   23   tcp    -  209.874367     0  ...   S1  -  -  0   \n",
       "6378290    192.121.45.63   23   tcp    -  165.760290     0  ...   S1  -  -  0   \n",
       "6378291  192.168.100.111    0  icmp    -           -     -  ...  OTH  -  -  0   \n",
       "6378292  192.168.100.111    0  icmp    -           -     -  ...  OTH  -  -  0   \n",
       "\n",
       "          15 16   17 18   19                                               20  \n",
       "0         Dd  1   76  1   76                             (empty)   Benign   -  \n",
       "1        DAd  5  332  2  200                             (empty)   Benign   -  \n",
       "2        SAD  3  189  0    0                             (empty)   Benign   -  \n",
       "3        SAD  3  189  0    0                             (empty)   Benign   -  \n",
       "4          D  1   58  0    0                             (empty)   Benign   -  \n",
       "...      ... ..  ... ..  ...                                              ...  \n",
       "6378288    S  1   40  0    0  (empty)   Malicious   PartOfAHorizontalPortScan  \n",
       "6378289  ShA  8  360  1   44                             (empty)   Benign   -  \n",
       "6378290  ShA  5  220  1   44                             (empty)   Benign   -  \n",
       "6378291    -  1   68  0    0                             (empty)   Benign   -  \n",
       "6378292    -  1   56  0    0                             (empty)   Benign   -  \n",
       "\n",
       "[6378293 rows x 21 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e95435cd-4505-4882-aba9-ca3db9b206ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['(empty)   Benign   -', '(empty)   Benign   -',\n",
       "       '(empty)   Benign   -', '(empty)   Benign   -',\n",
       "       '(empty)   Benign   -', '(empty)   Benign   -',\n",
       "       '(empty)   Benign   -', '(empty)   Benign   -',\n",
       "       '(empty)   Benign   -', '(empty)   Benign   -',\n",
       "       '(empty)   Benign   -', '(empty)   Benign   -',\n",
       "       '(empty)   Benign   -', '(empty)   Benign   -',\n",
       "       '(empty)   Benign   -', '(empty)   Benign   -',\n",
       "       '(empty)   Benign   -', '(empty)   Benign   -',\n",
       "       '(empty)   Benign   -', '(empty)   Benign   -',\n",
       "       '(empty)   Benign   -', '(empty)   Benign   -',\n",
       "       '(empty)   Benign   -', '(empty)   Benign   -',\n",
       "       '(empty)   Benign   -', '(empty)   Benign   -',\n",
       "       '(empty)   Benign   -', '(empty)   Benign   -',\n",
       "       '(empty)   Benign   -', '(empty)   Benign   -',\n",
       "       '(empty)   Benign   -', '(empty)   Benign   -',\n",
       "       '(empty)   Benign   -', '(empty)   Benign   -',\n",
       "       '(empty)   Benign   -', '(empty)   Benign   -',\n",
       "       '(empty)   Benign   -', '(empty)   Benign   -',\n",
       "       '(empty)   Benign   -', '(empty)   Benign   -',\n",
       "       '(empty)   Benign   -', '(empty)   Benign   -',\n",
       "       '(empty)   Benign   -', '(empty)   Benign   -',\n",
       "       '(empty)   Benign   -', '(empty)   Benign   -',\n",
       "       '(empty)   Benign   -', '(empty)   Benign   -',\n",
       "       '(empty)   Benign   -', '(empty)   Benign   -',\n",
       "       '(empty)   Benign   -', '(empty)   Benign   -',\n",
       "       '(empty)   Benign   -', '(empty)   Benign   -',\n",
       "       '(empty)   Benign   -', '(empty)   Benign   -',\n",
       "       '(empty)   Benign   -', '(empty)   Benign   -',\n",
       "       '(empty)   Benign   -', '(empty)   Benign   -',\n",
       "       '(empty)   Benign   -', '(empty)   Benign   -',\n",
       "       '(empty)   Benign   -', '(empty)   Benign   -',\n",
       "       '(empty)   Benign   -', '(empty)   Benign   -',\n",
       "       '(empty)   Benign   -', '(empty)   Benign   -',\n",
       "       '(empty)   Benign   -', '(empty)   Benign   -',\n",
       "       '(empty)   Benign   -', '(empty)   Benign   -',\n",
       "       '(empty)   Benign   -', '(empty)   Benign   -',\n",
       "       '(empty)   Benign   -', '(empty)   Benign   -',\n",
       "       '(empty)   Benign   -', '(empty)   Benign   -',\n",
       "       '(empty)   Benign   -', '(empty)   Benign   -',\n",
       "       '(empty)   Benign   -', '(empty)   Benign   -',\n",
       "       '(empty)   Benign   -', '(empty)   Benign   -',\n",
       "       '(empty)   Benign   -', '(empty)   Benign   -',\n",
       "       '(empty)   Benign   -', '(empty)   Benign   -',\n",
       "       '(empty)   Benign   -', '(empty)   Benign   -',\n",
       "       '(empty)   Benign   -', '(empty)   Benign   -',\n",
       "       '(empty)   Benign   -', '(empty)   Benign   -',\n",
       "       '(empty)   Benign   -', '(empty)   Benign   -',\n",
       "       '(empty)   Benign   -', '(empty)   Benign   -',\n",
       "       '(empty)   Benign   -'], dtype=object)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[20].values[1:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4a32e6e2-d2f9-4dc9-a8ca-303df1ac0238",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#fields',\n",
       " 'ts',\n",
       " 'uid',\n",
       " 'id.orig_h',\n",
       " 'id.orig_p',\n",
       " 'id.resp_h',\n",
       " 'id.resp_p',\n",
       " 'proto',\n",
       " 'service',\n",
       " 'duration',\n",
       " 'orig_bytes',\n",
       " 'resp_bytes',\n",
       " 'conn_state',\n",
       " 'local_orig',\n",
       " 'local_resp',\n",
       " 'missed_bytes',\n",
       " 'history',\n",
       " 'orig_pkts',\n",
       " 'orig_ip_bytes',\n",
       " 'resp_pkts',\n",
       " 'resp_ip_bytes',\n",
       " 'tunnel_parents   label   detailed-label\\n']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "688853ee-d913-473d-83c8-2ede8b07e0e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#fields'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c0ff98e8-8624-48b0-b1c8-de8b1dc67b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "x[20]='tunnel_parents   label   detailed-label'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a9450392-9e98-4889-b9df-23abd492e12f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ts',\n",
       " 'uid',\n",
       " 'id.orig_h',\n",
       " 'id.orig_p',\n",
       " 'id.resp_h',\n",
       " 'id.resp_p',\n",
       " 'proto',\n",
       " 'service',\n",
       " 'duration',\n",
       " 'orig_bytes',\n",
       " 'resp_bytes',\n",
       " 'conn_state',\n",
       " 'local_orig',\n",
       " 'local_resp',\n",
       " 'missed_bytes',\n",
       " 'history',\n",
       " 'orig_pkts',\n",
       " 'orig_ip_bytes',\n",
       " 'resp_pkts',\n",
       " 'resp_ip_bytes',\n",
       " 'tunnel_parents   label   detailed-label']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "94c6cde9-2de9-496d-8e38-e3228a2d9155",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns=x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "092af13a-181a-4644-977e-b3d1c4347f46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ts</th>\n",
       "      <th>uid</th>\n",
       "      <th>id.orig_h</th>\n",
       "      <th>id.orig_p</th>\n",
       "      <th>id.resp_h</th>\n",
       "      <th>id.resp_p</th>\n",
       "      <th>proto</th>\n",
       "      <th>service</th>\n",
       "      <th>duration</th>\n",
       "      <th>orig_bytes</th>\n",
       "      <th>...</th>\n",
       "      <th>conn_state</th>\n",
       "      <th>local_orig</th>\n",
       "      <th>local_resp</th>\n",
       "      <th>missed_bytes</th>\n",
       "      <th>history</th>\n",
       "      <th>orig_pkts</th>\n",
       "      <th>orig_ip_bytes</th>\n",
       "      <th>resp_pkts</th>\n",
       "      <th>resp_ip_bytes</th>\n",
       "      <th>tunnel_parents   label   detailed-label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1532510451.648888</td>\n",
       "      <td>CnR6zKxrWbFw26ua7</td>\n",
       "      <td>192.168.100.111</td>\n",
       "      <td>40008</td>\n",
       "      <td>46.28.110.244</td>\n",
       "      <td>123</td>\n",
       "      <td>udp</td>\n",
       "      <td>-</td>\n",
       "      <td>0.004751</td>\n",
       "      <td>48</td>\n",
       "      <td>...</td>\n",
       "      <td>SF</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>Dd</td>\n",
       "      <td>1</td>\n",
       "      <td>76</td>\n",
       "      <td>1</td>\n",
       "      <td>76</td>\n",
       "      <td>(empty)   Benign   -</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1532511003.118878</td>\n",
       "      <td>CcfURS3zgEMzX0RqTc</td>\n",
       "      <td>192.168.100.102</td>\n",
       "      <td>57849</td>\n",
       "      <td>192.168.100.111</td>\n",
       "      <td>22</td>\n",
       "      <td>tcp</td>\n",
       "      <td>-</td>\n",
       "      <td>108.421563</td>\n",
       "      <td>2376</td>\n",
       "      <td>...</td>\n",
       "      <td>OTH</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>DAd</td>\n",
       "      <td>5</td>\n",
       "      <td>332</td>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>(empty)   Benign   -</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1532511739.837566</td>\n",
       "      <td>CPhAqD1EhNOoPVA6ha</td>\n",
       "      <td>192.168.100.102</td>\n",
       "      <td>59670</td>\n",
       "      <td>192.168.100.111</td>\n",
       "      <td>22</td>\n",
       "      <td>tcp</td>\n",
       "      <td>-</td>\n",
       "      <td>0.001244</td>\n",
       "      <td>21</td>\n",
       "      <td>...</td>\n",
       "      <td>S0</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>SAD</td>\n",
       "      <td>3</td>\n",
       "      <td>189</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(empty)   Benign   -</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1532512077.774888</td>\n",
       "      <td>CFr56iYeYk4YZXrPf</td>\n",
       "      <td>192.168.100.102</td>\n",
       "      <td>59701</td>\n",
       "      <td>192.168.100.111</td>\n",
       "      <td>22</td>\n",
       "      <td>tcp</td>\n",
       "      <td>-</td>\n",
       "      <td>0.000738</td>\n",
       "      <td>21</td>\n",
       "      <td>...</td>\n",
       "      <td>S0</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>SAD</td>\n",
       "      <td>3</td>\n",
       "      <td>189</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(empty)   Benign   -</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1532512335.605565</td>\n",
       "      <td>C8a9xW23hF8nDgeNtj</td>\n",
       "      <td>192.168.100.111</td>\n",
       "      <td>36199</td>\n",
       "      <td>192.168.100.1</td>\n",
       "      <td>53</td>\n",
       "      <td>udp</td>\n",
       "      <td>dns</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>...</td>\n",
       "      <td>S0</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>D</td>\n",
       "      <td>1</td>\n",
       "      <td>58</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(empty)   Benign   -</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6378288</th>\n",
       "      <td>1532595195.007148</td>\n",
       "      <td>CIHonx2ymqlX5DqnU2</td>\n",
       "      <td>192.168.100.111</td>\n",
       "      <td>28057</td>\n",
       "      <td>173.94.58.125</td>\n",
       "      <td>23</td>\n",
       "      <td>tcp</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>...</td>\n",
       "      <td>S0</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(empty)   Malicious   PartOfAHorizontalPortScan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6378289</th>\n",
       "      <td>1532594748.215022</td>\n",
       "      <td>CwhtUD3BIdOP5L6Y8g</td>\n",
       "      <td>192.168.100.111</td>\n",
       "      <td>52876</td>\n",
       "      <td>69.196.96.231</td>\n",
       "      <td>23</td>\n",
       "      <td>tcp</td>\n",
       "      <td>-</td>\n",
       "      <td>209.874367</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>S1</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>ShA</td>\n",
       "      <td>8</td>\n",
       "      <td>360</td>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>(empty)   Benign   -</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6378290</th>\n",
       "      <td>1532591674.249972</td>\n",
       "      <td>CxHWyM2vp5x1mCZmp3</td>\n",
       "      <td>192.168.100.111</td>\n",
       "      <td>39234</td>\n",
       "      <td>192.121.45.63</td>\n",
       "      <td>23</td>\n",
       "      <td>tcp</td>\n",
       "      <td>-</td>\n",
       "      <td>165.760290</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>S1</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>ShA</td>\n",
       "      <td>5</td>\n",
       "      <td>220</td>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>(empty)   Benign   -</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6378291</th>\n",
       "      <td>1532595145.171589</td>\n",
       "      <td>CtmaRjXW7FOus0sPa</td>\n",
       "      <td>168.102.14.4</td>\n",
       "      <td>11</td>\n",
       "      <td>192.168.100.111</td>\n",
       "      <td>0</td>\n",
       "      <td>icmp</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>...</td>\n",
       "      <td>OTH</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>1</td>\n",
       "      <td>68</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(empty)   Benign   -</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6378292</th>\n",
       "      <td>1532595149.229773</td>\n",
       "      <td>CUVfp02K1YEXmtDf2h</td>\n",
       "      <td>192.94.118.248</td>\n",
       "      <td>11</td>\n",
       "      <td>192.168.100.111</td>\n",
       "      <td>0</td>\n",
       "      <td>icmp</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>...</td>\n",
       "      <td>OTH</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>1</td>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(empty)   Benign   -</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6378293 rows  21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        ts                 uid        id.orig_h id.orig_p  \\\n",
       "0        1532510451.648888   CnR6zKxrWbFw26ua7  192.168.100.111     40008   \n",
       "1        1532511003.118878  CcfURS3zgEMzX0RqTc  192.168.100.102     57849   \n",
       "2        1532511739.837566  CPhAqD1EhNOoPVA6ha  192.168.100.102     59670   \n",
       "3        1532512077.774888   CFr56iYeYk4YZXrPf  192.168.100.102     59701   \n",
       "4        1532512335.605565  C8a9xW23hF8nDgeNtj  192.168.100.111     36199   \n",
       "...                    ...                 ...              ...       ...   \n",
       "6378288  1532595195.007148  CIHonx2ymqlX5DqnU2  192.168.100.111     28057   \n",
       "6378289  1532594748.215022  CwhtUD3BIdOP5L6Y8g  192.168.100.111     52876   \n",
       "6378290  1532591674.249972  CxHWyM2vp5x1mCZmp3  192.168.100.111     39234   \n",
       "6378291  1532595145.171589   CtmaRjXW7FOus0sPa     168.102.14.4        11   \n",
       "6378292  1532595149.229773  CUVfp02K1YEXmtDf2h   192.94.118.248        11   \n",
       "\n",
       "               id.resp_h id.resp_p proto service    duration orig_bytes  ...  \\\n",
       "0          46.28.110.244       123   udp       -    0.004751         48  ...   \n",
       "1        192.168.100.111        22   tcp       -  108.421563       2376  ...   \n",
       "2        192.168.100.111        22   tcp       -    0.001244         21  ...   \n",
       "3        192.168.100.111        22   tcp       -    0.000738         21  ...   \n",
       "4          192.168.100.1        53   udp     dns           -          -  ...   \n",
       "...                  ...       ...   ...     ...         ...        ...  ...   \n",
       "6378288    173.94.58.125        23   tcp       -           -          -  ...   \n",
       "6378289    69.196.96.231        23   tcp       -  209.874367          0  ...   \n",
       "6378290    192.121.45.63        23   tcp       -  165.760290          0  ...   \n",
       "6378291  192.168.100.111         0  icmp       -           -          -  ...   \n",
       "6378292  192.168.100.111         0  icmp       -           -          -  ...   \n",
       "\n",
       "        conn_state local_orig local_resp missed_bytes history orig_pkts  \\\n",
       "0               SF          -          -            0      Dd         1   \n",
       "1              OTH          -          -            0     DAd         5   \n",
       "2               S0          -          -            0     SAD         3   \n",
       "3               S0          -          -            0     SAD         3   \n",
       "4               S0          -          -            0       D         1   \n",
       "...            ...        ...        ...          ...     ...       ...   \n",
       "6378288         S0          -          -            0       S         1   \n",
       "6378289         S1          -          -            0     ShA         8   \n",
       "6378290         S1          -          -            0     ShA         5   \n",
       "6378291        OTH          -          -            0       -         1   \n",
       "6378292        OTH          -          -            0       -         1   \n",
       "\n",
       "        orig_ip_bytes resp_pkts resp_ip_bytes  \\\n",
       "0                  76         1            76   \n",
       "1                 332         2           200   \n",
       "2                 189         0             0   \n",
       "3                 189         0             0   \n",
       "4                  58         0             0   \n",
       "...               ...       ...           ...   \n",
       "6378288            40         0             0   \n",
       "6378289           360         1            44   \n",
       "6378290           220         1            44   \n",
       "6378291            68         0             0   \n",
       "6378292            56         0             0   \n",
       "\n",
       "                 tunnel_parents   label   detailed-label  \n",
       "0                                   (empty)   Benign   -  \n",
       "1                                   (empty)   Benign   -  \n",
       "2                                   (empty)   Benign   -  \n",
       "3                                   (empty)   Benign   -  \n",
       "4                                   (empty)   Benign   -  \n",
       "...                                                  ...  \n",
       "6378288  (empty)   Malicious   PartOfAHorizontalPortScan  \n",
       "6378289                             (empty)   Benign   -  \n",
       "6378290                             (empty)   Benign   -  \n",
       "6378291                             (empty)   Benign   -  \n",
       "6378292                             (empty)   Benign   -  \n",
       "\n",
       "[6378293 rows x 21 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8bb9bcfe-2b7c-4cc1-a9c3-d66b3cb2151d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"9.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a358ad4b-110c-4eda-98e7-ec997f419adf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
